{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3f9a4154",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9a4154",
        "outputId": "28a2d021-12e9-40fa-cc2b-933b0d028714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install datasets huggingface_hub transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "00b1fec3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00b1fec3",
        "outputId": "8d09da04-8d43-4cb9-f14a-78522348b4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset exists!\n",
            "Dataset ID: Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\n",
            "Tags: ['task_categories:text-generation', 'task_categories:question-answering', 'language:en', 'license:apache-2.0', 'size_categories:10K<n<100K', 'format:json', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'region:us', 'cybersecurity', 'defensive-security', 'instruction-tuning', 'threat-intelligence', 'incident-response', 'security-operations']\n",
            "Number of files: 3\n",
            "\n",
            "Files in repository:\n",
            "  - .gitattributes\n",
            "  - CyberSec-Dataset_escaped.jsonl\n",
            "  - README.md\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, dataset_info\n",
        "import requests\n",
        "\n",
        "# Check the dataset repository structure\n",
        "api = HfApi()\n",
        "try:\n",
        "    info = api.dataset_info(\"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\")\n",
        "    print(\"Dataset exists!\")\n",
        "    print(f\"Dataset ID: {info.id}\")\n",
        "    print(f\"Tags: {info.tags}\")\n",
        "    print(f\"Number of files: {len(info.siblings) if info.siblings else 0}\")\n",
        "\n",
        "    if info.siblings:\n",
        "        print(\"\\nFiles in repository:\")\n",
        "        for file in info.siblings:\n",
        "            print(f\"  - {file.rfilename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing dataset info: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "7a480d5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a480d5e",
        "outputId": "83190c2c-11ef-4816-dde0-2cdf64ff5651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method 1: Loading with explicit data_files...\n",
            "✅ Method 1 SUCCESS!\n",
            "Dataset loaded with 53201 examples\n",
            "Features: {'system': Value('string'), 'user': Value('string'), 'assistant': Value('string')}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Method 1: Try loading with explicit data_files parameter\n",
        "print(\"Method 1: Loading with explicit data_files...\")\n",
        "try:\n",
        "    ds1 = load_dataset(\n",
        "        \"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\",\n",
        "        data_files=\"CyberSec-Dataset_escaped.jsonl\"\n",
        "    )\n",
        "    print(\"✅ Method 1 SUCCESS!\")\n",
        "    print(f\"Dataset loaded with {len(ds1['train'])} examples\")\n",
        "    print(f\"Features: {ds1['train'].features}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Method 1 FAILED: {e}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "6982d110",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6982d110",
        "outputId": "68198eaa-6365-4ee7-d8d6-cf4f50a39fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method 2: Loading with explicit format...\n",
            "✅ Method 2 SUCCESS!\n",
            "Dataset loaded with 53201 examples\n",
            "Method 3: Loading with streaming...\n",
            "✅ Method 3 SUCCESS!\n",
            "Streaming dataset created successfully\n",
            "Streaming sample taken: 100 examples\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Try specifying the file format explicitly\n",
        "print(\"Method 2: Loading with explicit format...\")\n",
        "try:\n",
        "    ds2 = load_dataset(\n",
        "        \"json\",\n",
        "        data_files=\"hf://datasets/Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset/CyberSec-Dataset_escaped.jsonl\"\n",
        "    )\n",
        "    print(\"✅ Method 2 SUCCESS!\")\n",
        "    print(f\"Dataset loaded with {len(ds2['train'])} examples\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Method 2 FAILED: {e}\")\n",
        "    print()\n",
        "\n",
        "# Method 3: Try with streaming=True to avoid local caching issues\n",
        "print(\"Method 3: Loading with streaming...\")\n",
        "try:\n",
        "    ds3 = load_dataset(\n",
        "        \"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\",\n",
        "        data_files=\"CyberSec-Dataset_escaped.jsonl\",\n",
        "        streaming=True\n",
        "    )\n",
        "    print(\"✅ Method 3 SUCCESS!\")\n",
        "    print(\"Streaming dataset created successfully\")\n",
        "    # Convert to regular dataset for consistency\n",
        "    ds3_regular = ds3['train'].take(100)  # Take first 100 examples as test\n",
        "    print(f\"Streaming sample taken: {len(list(ds3_regular))} examples\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Method 3 FAILED: {e}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "4ba727f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ba727f2",
        "outputId": "d6863feb-fe44-4e10-ef88-ce67473587c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATASET SAMPLE ===\n",
            "Total examples: 53201\n",
            "Features: ['system', 'user', 'assistant']\n",
            "\n",
            "Example 1:\n",
            "System: You are a highly specialized AI assistant for advanced cyber-defense whose mission is to deliver accurate, in-depth, actionable guidance on information-security principles—confidentiality, integrity, ...\n",
            "User: Analyze encrypted C2 channels using TLS. Discuss traffic analysis techniques to fingerprint malicious sessions.\n",
            "Assistant: Encrypted Command and Control (C2) channels utilizing Transport Layer Security (TLS) present significant challenges for network defenders, as traditional packet inspection methods cannot directly anal...\n",
            "\n",
            "=== BASIC STATISTICS ===\n",
            "Average system prompt length: 1085.0 chars\n",
            "Average user message length: 145.7 chars\n",
            "Average assistant response length: 2084.9 chars\n"
          ]
        }
      ],
      "source": [
        "# Display sample data from the successfully loaded dataset\n",
        "print(\"=== DATASET SAMPLE ===\")\n",
        "print(f\"Total examples: {len(ds1['train'])}\")\n",
        "print(f\"Features: {list(ds1['train'].features.keys())}\")\n",
        "print()\n",
        "\n",
        "# Show first example\n",
        "sample = ds1['train'][0]\n",
        "print(\"Example 1:\")\n",
        "print(f\"System: {sample['system'][:200]}...\" if len(sample['system']) > 200 else f\"System: {sample['system']}\")\n",
        "print(f\"User: {sample['user'][:200]}...\" if len(sample['user']) > 200 else f\"User: {sample['user']}\")\n",
        "print(f\"Assistant: {sample['assistant'][:200]}...\" if len(sample['assistant']) > 200 else f\"Assistant: {sample['assistant']}\")\n",
        "print()\n",
        "\n",
        "# Show data distribution\n",
        "print(\"=== BASIC STATISTICS ===\")\n",
        "sample_data = ds1['train'].select(range(min(1000, len(ds1['train']))))  # Sample first 1000 or total length\n",
        "system_lengths = [len(ex) for ex in sample_data['system']]\n",
        "user_lengths = [len(ex) for ex in sample_data['user']]\n",
        "assistant_lengths = [len(ex) for ex in sample_data['assistant']]\n",
        "\n",
        "print(f\"Average system prompt length: {sum(system_lengths)/len(system_lengths):.1f} chars\")\n",
        "print(f\"Average user message length: {sum(user_lengths)/len(user_lengths):.1f} chars\")\n",
        "print(f\"Average assistant response length: {sum(assistant_lengths)/len(assistant_lengths):.1f} chars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "43edf48f",
      "metadata": {
        "id": "43edf48f"
      },
      "outputs": [],
      "source": [
        "# First, let's fix the quantization issue by updating the model loading approach\n",
        "!pip install -q bitsandbytes accelerate\n",
        "\n",
        "# Also ensure we have compatible versions\n",
        "!pip install -q transformers>=4.36.0 peft>=0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c2c7765b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2c7765b",
        "outputId": "c4bd24e8-a777-412e-e1c6-00c69a9c2355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed CybersecurityFineTuner class created!\n"
          ]
        }
      ],
      "source": [
        "# set up training\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "class CybersecurityFineTunerFixed:\n",
        "    \"\"\"Fixed fine-tuning class for cybersecurity-specific models.\"\"\"\n",
        "\n",
        "    def __init__(self, config: dict):\n",
        "        \"\"\"Initialize the fine-tuner with configuration.\"\"\"\n",
        "        self.config = config\n",
        "        self.model_name = config.get('model_name', 'microsoft/DialoGPT-medium')  # Using a smaller model for testing\n",
        "        self.output_dir = config.get('output_dir', './cybersecurity-lora')\n",
        "        self.max_length = config.get('max_length', 512)\n",
        "\n",
        "        # Check for CUDA availability\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize components\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def load_tokenizer(self):\n",
        "        \"\"\"Load and configure the tokenizer.\"\"\"\n",
        "        print(f\"Loading tokenizer from {self.model_name}\")\n",
        "\n",
        "        from transformers import AutoTokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Set pad token if not present\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            print(\"Set pad_token to eos_token\")\n",
        "\n",
        "        print(\"Tokenizer loaded successfully\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the base model with proper quantization configuration.\"\"\"\n",
        "        print(f\"Loading model from {self.model_name}\")\n",
        "\n",
        "        from transformers import AutoModelForCausalLM\n",
        "\n",
        "        # Proper quantization configuration\n",
        "        if self.device == \"cuda\":\n",
        "            # Create proper BitsAndBytesConfig\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                bnb_8bit_compute_dtype=torch.float16,\n",
        "                bnb_8bit_use_double_quant=True,\n",
        "            )\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            print(\"Model loaded with 8-bit quantization\")\n",
        "        else:\n",
        "            # No quantization for CPU\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=torch.float16,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            print(\"Model loaded without quantization (CPU)\")\n",
        "\n",
        "        print(f\"Model loaded on device: {next(self.model.parameters()).device}\")\n",
        "\n",
        "    def configure_lora(self):\n",
        "        \"\"\"Configure and apply LoRA to the model.\"\"\"\n",
        "        print(\"Configuring LoRA\")\n",
        "\n",
        "        from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "        # Get target modules based on the model architecture\n",
        "        target_modules = []\n",
        "        if \"gpt\" in self.model_name.lower():\n",
        "            target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
        "        elif \"llama\" in self.model_name.lower():\n",
        "            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "        else:\n",
        "            # Generic targets that work for most transformer models\n",
        "            target_modules = [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=self.config.get('lora_r', 16),\n",
        "            lora_alpha=self.config.get('lora_alpha', 32),\n",
        "            lora_dropout=self.config.get('lora_dropout', 0.1),\n",
        "            target_modules=target_modules,\n",
        "        )\n",
        "\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        # Print trainable parameters\n",
        "        self.model.print_trainable_parameters()\n",
        "        print(\"LoRA configuration applied\")\n",
        "\n",
        "    def prepare_streaming_dataset(self, streaming_dataset):\n",
        "        \"\"\"Convert streaming dataset to regular dataset for training.\"\"\"\n",
        "        print(\"Converting streaming dataset to regular dataset...\")\n",
        "\n",
        "        # Take a subset for training (you can adjust this number)\n",
        "        max_samples = self.config.get('max_samples', 1000)\n",
        "\n",
        "        # Convert iterable dataset to list\n",
        "        train_data = []\n",
        "        count = 0\n",
        "        for example in streaming_dataset['train']:\n",
        "            if count >= max_samples:\n",
        "                break\n",
        "            train_data.append(example)\n",
        "            count += 1\n",
        "\n",
        "            if count % 100 == 0:\n",
        "                print(f\"Processed {count} examples...\")\n",
        "\n",
        "        # Convert to Hugging Face Dataset\n",
        "        from datasets import Dataset\n",
        "        dataset = Dataset.from_list(train_data)\n",
        "\n",
        "        print(f\"Converted {len(dataset)} examples to regular dataset\")\n",
        "        return dataset\n",
        "\n",
        "    def format_dataset(self, dataset):\n",
        "        \"\"\"Format the dataset for instruction following.\"\"\"\n",
        "        print(\"Formatting dataset\")\n",
        "\n",
        "        def format_prompt(example):\n",
        "            \"\"\"Format examples for instruction following.\"\"\"\n",
        "            # Handle the cybersecurity dataset format (system, user, assistant)\n",
        "            if \"system\" in example and \"user\" in example and \"assistant\" in example:\n",
        "                # Create a chat-like format\n",
        "                prompt = f\"System: {example['system']}\\n\\nUser: {example['user']}\\n\\nAssistant: {example['assistant']}\"\n",
        "            elif \"instruction\" in example and \"response\" in example:\n",
        "                prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
        "            elif \"question\" in example and \"answer\" in example:\n",
        "                prompt = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n",
        "            else:\n",
        "                # Fallback: convert to string\n",
        "                prompt = str(example)\n",
        "\n",
        "            return {\"text\": prompt}\n",
        "\n",
        "        # Apply formatting\n",
        "        formatted_dataset = dataset.map(format_prompt)\n",
        "        print(\"Dataset formatting completed\")\n",
        "        return formatted_dataset\n",
        "\n",
        "    def tokenize_dataset(self, dataset):\n",
        "        \"\"\"Tokenize the dataset for training.\"\"\"\n",
        "        print(\"Tokenizing dataset\")\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            \"\"\"Tokenize the dataset for training.\"\"\"\n",
        "            return self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                padding=False,\n",
        "                max_length=self.max_length,\n",
        "                return_overflowing_tokens=False,\n",
        "            )\n",
        "\n",
        "        # Tokenize the dataset\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names,\n",
        "            desc=\"Tokenizing dataset\"\n",
        "        )\n",
        "\n",
        "        # Create train/validation split\n",
        "        split_ratio = self.config.get('validation_split', 0.1)\n",
        "        if split_ratio > 0:\n",
        "            split_dataset = tokenized_dataset.train_test_split(test_size=split_ratio)\n",
        "            train_dataset = split_dataset[\"train\"]\n",
        "            eval_dataset = split_dataset[\"test\"]\n",
        "        else:\n",
        "            train_dataset = tokenized_dataset\n",
        "            eval_dataset = None\n",
        "\n",
        "        print(f\"Training samples: {len(train_dataset)}\")\n",
        "        if eval_dataset:\n",
        "            print(f\"Validation samples: {len(eval_dataset)}\")\n",
        "\n",
        "        return train_dataset, eval_dataset\n",
        "\n",
        "    def train_streaming(self, streaming_dataset):\n",
        "        \"\"\"Execute the complete training pipeline with streaming dataset.\"\"\"\n",
        "        print(\"Starting training pipeline with streaming dataset\")\n",
        "\n",
        "        # Load components\n",
        "        self.load_tokenizer()\n",
        "        self.load_model()\n",
        "        self.configure_lora()\n",
        "\n",
        "        # Prepare dataset\n",
        "        dataset = self.prepare_streaming_dataset(streaming_dataset)\n",
        "        formatted_dataset = self.format_dataset(dataset)\n",
        "        train_dataset, eval_dataset = self.tokenize_dataset(formatted_dataset)\n",
        "\n",
        "        # Create trainer\n",
        "        from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "        # Training arguments\n",
        "        # Removed evaluation_strategy as it is causing TypeError in this environment\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            per_device_train_batch_size=self.config.get('batch_size', 1),\n",
        "            per_device_eval_batch_size=self.config.get('eval_batch_size', 1),\n",
        "            gradient_accumulation_steps=self.config.get('gradient_accumulation_steps', 4),\n",
        "            num_train_epochs=self.config.get('epochs', 1),\n",
        "            learning_rate=self.config.get('learning_rate', 2e-4),\n",
        "            fp16=self.device == \"cuda\",\n",
        "            logging_steps=self.config.get('logging_steps', 10),\n",
        "            eval_steps=self.config.get('eval_steps', 100),\n",
        "            save_steps=self.config.get('save_steps', 500),\n",
        "            save_strategy=\"steps\",\n",
        "            load_best_model_at_end=False, # Set load_best_model_at_end to False to avoid strategy mismatch error\n",
        "            warmup_steps=self.config.get('warmup_steps', 50),\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            report_to=None,\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "        )\n",
        "\n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "        print(\"Starting training...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save model\n",
        "        print(\"Saving model...\")\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "\n",
        "        print(\"Training completed successfully!\")\n",
        "\n",
        "print(\"Fixed CybersecurityFineTuner class created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "51f44eb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51f44eb1",
        "outputId": "ee761a07-b87a-4465-f2c2-98f31481c2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration created for fixed fine-tuner\n",
            "Will use model: microsoft/DialoGPT-medium\n",
            "Max samples for training: 500\n",
            "Device: CUDA\n"
          ]
        }
      ],
      "source": [
        "# Create a configuration for the fixed fine-tuner\n",
        "config_fixed = {\n",
        "    \"model_name\": \"microsoft/DialoGPT-medium\",  # Using a smaller, more compatible model\n",
        "    \"output_dir\": \"./cybersecurity-lora-fixed\",\n",
        "    \"max_length\": 512,\n",
        "    \"max_samples\": 500,  # Limit samples for testing\n",
        "    \"validation_split\": 0.1,\n",
        "    \"lora_r\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.1,\n",
        "    \"batch_size\": 1,\n",
        "    \"eval_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"epochs\": 1,  # Just 1 epoch for testing\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"logging_steps\": 10,\n",
        "    \"eval_steps\": 50,\n",
        "    \"save_steps\": 100,\n",
        "    \"warmup_steps\": 20,\n",
        "}\n",
        "\n",
        "print(\"Configuration created for fixed fine-tuner\")\n",
        "print(f\"Will use model: {config_fixed['model_name']}\")\n",
        "print(f\"Max samples for training: {config_fixed['max_samples']}\")\n",
        "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "41a243aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41a243aa",
        "outputId": "4e3cdaf2-5ab1-4ef0-a73d-c5cc36a09b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing the fixed fine-tuner...\n",
            "Using streaming dataset: <class 'datasets.dataset_dict.IterableDatasetDict'>\n",
            "Dataset splits: ['train']\n",
            "Using device: cuda\n",
            "\n",
            "=== Testing dataset preparation ===\n",
            "Converting streaming dataset to regular dataset...\n",
            "Processed 100 examples...\n",
            "Processed 200 examples...\n",
            "Processed 300 examples...\n",
            "Processed 400 examples...\n",
            "Processed 500 examples...\n",
            "Converted 500 examples to regular dataset\n",
            "Dataset conversion successful! Got 500 examples\n",
            "\n",
            "Sample data fields: ['system', 'user', 'assistant']\n",
            "Sample system (first 100 chars): You are a highly specialized AI assistant for advanced cyber-defense whose mission is to deliver acc...\n",
            "Sample user (first 100 chars): Analyze encrypted C2 channels using TLS. Discuss traffic analysis techniques to fingerprint maliciou...\n",
            "Sample assistant (first 100 chars): Encrypted Command and Control (C2) channels utilizing Transport Layer Security (TLS) present signifi...\n"
          ]
        }
      ],
      "source": [
        "# Test the fixed fine-tuner with your streaming dataset\n",
        "print(\"Testing the fixed fine-tuner...\")\n",
        "print(f\"Using streaming dataset: {type(ds3)}\")\n",
        "print(f\"Dataset splits: {list(ds3.keys())}\")\n",
        "\n",
        "# Create the fixed fine-tuner\n",
        "fine_tuner_fixed = CybersecurityFineTunerFixed(config_fixed)\n",
        "\n",
        "# Let's first test the dataset preparation without training\n",
        "print(\"\\n=== Testing dataset preparation ===\")\n",
        "test_dataset = fine_tuner_fixed.prepare_streaming_dataset(ds3)\n",
        "print(f\"Dataset conversion successful! Got {len(test_dataset)} examples\")\n",
        "\n",
        "# Show a sample\n",
        "if len(test_dataset) > 0:\n",
        "    sample = test_dataset[0]\n",
        "    print(f\"\\nSample data fields: {list(sample.keys())}\")\n",
        "    print(f\"Sample system (first 100 chars): {sample['system'][:100]}...\")\n",
        "    print(f\"Sample user (first 100 chars): {sample['user'][:100]}...\")\n",
        "    print(f\"Sample assistant (first 100 chars): {sample['assistant'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "26d9a46f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788,
          "referenced_widgets": [
            "8f63c369136448db9a825072ffa99a2e",
            "9e79f7d2fa1946449a783e0bcf0efc54",
            "69cde02361bb4cc385d1643b80ba974c",
            "d39950dfedca485b94dd42b87b6e41e9",
            "a52b650931ed44ed910c512cc211718d",
            "a7800398017e4349844036aee2e167b1",
            "3797d944d3464c7eaf145c7522e71fe5",
            "9a3f28431e5f4673baa851c4429ab036",
            "2f8ff8edb8f54df3b3a2a15068369968",
            "2b5c14dd1fd944f59171802d7579953f",
            "f8f652878b194d25b9196d2503e50d7e",
            "932a0ac8cd704c25b98fc6d0dfc4b8c7",
            "45900c73cef94ae6ac970907c0ca9ac0",
            "5ddbd4bf29134a749616ec77ab8e29f8",
            "5898dede3dad48329c33edfb04b61736",
            "0ef499c3403c4e4197839141a871887b",
            "aee6356d647145d2ae10e32a748552a5",
            "6d17fa1771144488ba7cb0c96180e276",
            "eb0c99a1690244cebc73fc33325989fd",
            "ebc1acc527874dea931e501f04e4a79f",
            "12406203214b41b499a3306a0f92993a",
            "17e58f21f0444015a419c003ce4f0c35"
          ]
        },
        "id": "26d9a46f",
        "outputId": "2a2dacc4-49ba-48fa-f37d-04884831a357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Starting actual training ===\n",
            "Creating fine-tuner with small config for testing...\n",
            "Using device: cuda\n",
            "Starting training (this may take a few minutes even with small dataset)...\n",
            "Starting training pipeline with streaming dataset\n",
            "Loading tokenizer from microsoft/DialoGPT-medium\n",
            "Set pad_token to eos_token\n",
            "Tokenizer loaded successfully\n",
            "Loading model from microsoft/DialoGPT-medium\n",
            "Model loaded with 8-bit quantization\n",
            "Model loaded on device: cuda:0\n",
            "Configuring LoRA\n",
            "trainable params: 6,291,456 || all params: 361,114,624 || trainable%: 1.7422\n",
            "LoRA configuration applied\n",
            "Converting streaming dataset to regular dataset...\n",
            "Converted 50 examples to regular dataset\n",
            "Formatting dataset\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f63c369136448db9a825072ffa99a2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset formatting completed\n",
            "Tokenizing dataset\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "932a0ac8cd704c25b98fc6d0dfc4b8c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 45\n",
            "Validation samples: 5\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-92152699.py:238: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23/23 00:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>9.788900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>8.542300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>7.682600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.506500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "Training completed successfully!\n",
            "✅ Training completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Now let's run the actual training with a very small setup for testing\n",
        "print(\"=== Starting actual training ===\")\n",
        "\n",
        "# Create a smaller config for quick testing\n",
        "config_small = config_fixed.copy()\n",
        "config_small.update({\n",
        "    \"max_samples\": 50,  # Very small for quick test\n",
        "    \"batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 2,\n",
        "    \"epochs\": 1,\n",
        "    \"logging_steps\": 5,\n",
        "    \"eval_steps\": 20,\n",
        "    \"save_steps\": 50,\n",
        "    \"warmup_steps\": 5,\n",
        "})\n",
        "\n",
        "print(\"Creating fine-tuner with small config for testing...\")\n",
        "fine_tuner_small = CybersecurityFineTunerFixed(config_small)\n",
        "\n",
        "print(\"Starting training (this may take a few minutes even with small dataset)...\")\n",
        "try:\n",
        "    fine_tuner_small.train_streaming(ds3)\n",
        "    print(\"✅ Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Training failed with error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GMVOekNVKQtc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e05943536674457c8065faf190dfec9f",
            "e64bf29eecac4f67bff8f7dba7aa597b",
            "4aa453fba7b44dc2b693b8f898612f4d",
            "de364798ceaa48f7ad0b68fceff6990b",
            "7c51412832cb4959a597d55335585044",
            "b4a594e83b31411f86b8ed2144d08ef7",
            "2b0f23fcb93943728092124f36de69f6",
            "7d7a8b33961f474d933996fefa9ccd95",
            "b7099599574841d6a055d8406bd8bf41",
            "18f4a3f218ce4eddb95124cace94c9f7",
            "5948a69ced1b4b698fe6c2c6845a7fe4",
            "dfcdc0bdd32a476b83fa60bdbc41c233",
            "20d47ecaa0604f5083c51a971a001bf6",
            "12bb74b2440a488cbf4c8b6b17912692",
            "af877972c24b45d09bc7c975f12f8959",
            "199dad8114f647a7b20346e147644198",
            "ae6c82c22fff43039c93db25d0eafb0d",
            "5529c96ffc5f461e982f5f41ac821587",
            "c55ef04870864ba48fba394f05600762",
            "017c113f77d045a9a87b2f3e33241313",
            "814d7bdd5e02459dad0402cbad5ec660",
            "5c3b3768f46c48c488a3aacab87bffb7"
          ]
        },
        "id": "GMVOekNVKQtc",
        "outputId": "de3f5813-884b-4b2c-ad1e-eac44c13c611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Starting actual training (full dataset) ===\n",
            "Configuration created for training fine-tuner\n",
            "Will use model: microsoft/DialoGPT-medium\n",
            "Device: CUDA\n",
            "Creating fine-tuner with regular config for training...\n",
            "Using device: cuda\n",
            "Starting training (this may take a few minutes even with small dataset)...\n",
            "Starting training pipeline with streaming dataset\n",
            "Loading tokenizer from microsoft/DialoGPT-medium\n",
            "Set pad_token to eos_token\n",
            "Tokenizer loaded successfully\n",
            "Loading model from microsoft/DialoGPT-medium\n",
            "Model loaded with 8-bit quantization\n",
            "Model loaded on device: cuda:0\n",
            "Configuring LoRA\n",
            "trainable params: 6,291,456 || all params: 361,114,624 || trainable%: 1.7422\n",
            "LoRA configuration applied\n",
            "Converting streaming dataset to regular dataset...\n",
            "Processed 100 examples...\n",
            "Processed 200 examples...\n",
            "Processed 300 examples...\n",
            "Processed 400 examples...\n",
            "Processed 500 examples...\n",
            "Converted 500 examples to regular dataset\n",
            "Formatting dataset\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e05943536674457c8065faf190dfec9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset formatting completed\n",
            "Tokenizing dataset\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfcdc0bdd32a476b83fa60bdbc41c233",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 450\n",
            "Validation samples: 50\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-92152699.py:238: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='202' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [202/339 04:14 < 02:54, 0.78 it/s, Epoch 1.78/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>9.786000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>8.234300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>6.967100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.285700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>4.505100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.676900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.156500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.944300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.821800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.793600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.727500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.648400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.452500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.523300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.401200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.466600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.372200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.382800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        }
      ],
      "source": [
        "# Now let's run the actual training with the full dataset\n",
        "print(\"=== Starting actual training (full dataset) ===\")\n",
        "\n",
        "# Create a configuration for the training fine-tuner\n",
        "config_train = config_fixed.copy()\n",
        "config_train.update({\n",
        "    \"model_name\": \"microsoft/DialoGPT-medium\",  # Using a smaller, more compatible model\n",
        "    \"output_dir\": \"./cybersecurity-lora-fixed\",\n",
        "    \"max_length\": 512,\n",
        "    # \"max_samples\": 500,  # Limit samples for testing\n",
        "    \"validation_split\": 0.1,\n",
        "    \"lora_r\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.1,\n",
        "    \"batch_size\": 1,\n",
        "    \"eval_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"epochs\": 3,  # Just 1 epoch for testing\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"logging_steps\": 10,\n",
        "    \"eval_steps\": 50,\n",
        "    \"save_steps\": 100,\n",
        "    \"warmup_steps\": 20,\n",
        "})\n",
        "\n",
        "print(\"Configuration created for training fine-tuner\")\n",
        "print(f\"Will use model: {config_fixed['model_name']}\")\n",
        "# print(f\"Max samples for training: {config_fixed['max_samples']}\")\n",
        "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "print(\"Creating fine-tuner with regular config for training...\")\n",
        "fine_tuner = CybersecurityFineTunerFixed(config_train)\n",
        "\n",
        "print(\"Starting training (this may take a few minutes even with small dataset)...\")\n",
        "try:\n",
        "    fine_tuner.train_streaming(ds3)\n",
        "    print(\"✅ Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Training failed with error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736e8e8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # SOLUTION FOR GOOGLE COLAB\n",
        "# print(\"=== SOLUTIONS FOR GOOGLE COLAB ===\")\n",
        "# print()\n",
        "\n",
        "# print(\"1. TORCH VERSION ISSUE:\")\n",
        "# print(\"In Google Colab, run this first:\")\n",
        "# print(\"!pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
        "# print(\"Then restart runtime!\")\n",
        "# print()\n",
        "\n",
        "# print(\"2. QUANTIZATION ISSUE FIX:\")\n",
        "# print(\"For Google Colab with CUDA, use this updated code:\")\n",
        "# print()\n",
        "\n",
        "# # Provide the corrected Colab-specific code\n",
        "# colab_code = '''\n",
        "# # FOR GOOGLE COLAB - Run this in your Colab notebook:\n",
        "\n",
        "# # 1. First install/upgrade packages\n",
        "# !pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install -q transformers>=4.40.0 peft>=0.7.0 bitsandbytes accelerate datasets\n",
        "\n",
        "# # 2. Restart runtime, then use this code:\n",
        "\n",
        "# from transformers import BitsAndBytesConfig\n",
        "# import torch\n",
        "\n",
        "# # Fixed quantization config for newer transformers\n",
        "# def create_quantization_config():\n",
        "#     return BitsAndBytesConfig(\n",
        "#         load_in_8bit=True,\n",
        "#         bnb_8bit_compute_dtype=torch.float16,\n",
        "#         bnb_8bit_quant_type=\"nf8\",\n",
        "#         bnb_8bit_use_double_quant=True,\n",
        "#     )\n",
        "\n",
        "# # Updated model loading for Colab\n",
        "# def load_model_colab(model_name):\n",
        "#     from transformers import AutoModelForCausalLM\n",
        "\n",
        "#     if torch.cuda.is_available():\n",
        "#         quantization_config = create_quantization_config()\n",
        "#         model = AutoModelForCausalLM.from_pretrained(\n",
        "#             model_name,\n",
        "#             quantization_config=quantization_config,\n",
        "#             device_map=\"auto\",\n",
        "#             torch_dtype=torch.float16,\n",
        "#             trust_remote_code=True,\n",
        "#             use_safetensors=True,  # This helps with the security issue\n",
        "#         )\n",
        "#     else:\n",
        "#         model = AutoModelForCausalLM.from_pretrained(\n",
        "#             model_name,\n",
        "#             torch_dtype=torch.float16,\n",
        "#             trust_remote_code=True,\n",
        "#             use_safetensors=True,\n",
        "#         )\n",
        "#     return model\n",
        "\n",
        "# # 3. For your cybersecurity dataset, use this simple approach:\n",
        "# def simple_train_with_streaming_dataset(ds, model_name=\"microsoft/DialoGPT-medium\"):\n",
        "#     from datasets import Dataset\n",
        "#     from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "#     from transformers import DataCollatorForLanguageModeling\n",
        "#     from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "#     # Convert streaming to regular dataset (first 1000 examples)\n",
        "#     train_data = []\n",
        "#     for i, example in enumerate(ds['train']):\n",
        "#         if i >= 1000:  # Limit for memory\n",
        "#             break\n",
        "#         # Format the data\n",
        "#         text = f\"System: {example['system']}\\\\n\\\\nUser: {example['user']}\\\\n\\\\nAssistant: {example['assistant']}\"\n",
        "#         train_data.append({\"text\": text})\n",
        "\n",
        "#     dataset = Dataset.from_list(train_data)\n",
        "#     print(f\"Prepared {len(dataset)} examples\")\n",
        "\n",
        "#     # Load tokenizer and model\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#     if tokenizer.pad_token is None:\n",
        "#         tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#     model = load_model_colab(model_name)\n",
        "\n",
        "#     # Apply LoRA\n",
        "#     lora_config = LoraConfig(\n",
        "#         task_type=TaskType.CAUSAL_LM,\n",
        "#         r=16,\n",
        "#         lora_alpha=32,\n",
        "#         lora_dropout=0.1,\n",
        "#         target_modules=[\"c_attn\", \"c_proj\"] if \"gpt\" in model_name.lower() else [\"q_proj\", \"v_proj\"],\n",
        "#     )\n",
        "#     model = get_peft_model(model, lora_config)\n",
        "\n",
        "#     # Tokenize dataset\n",
        "#     def tokenize_function(examples):\n",
        "#         return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=512)\n",
        "\n",
        "#     tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "#     # Training\n",
        "#     training_args = TrainingArguments(\n",
        "#         output_dir=\"./cybersecurity-lora\",\n",
        "#         per_device_train_batch_size=1,\n",
        "#         gradient_accumulation_steps=4,\n",
        "#         num_train_epochs=1,\n",
        "#         learning_rate=2e-4,\n",
        "#         fp16=True if torch.cuda.is_available() else False,\n",
        "#         logging_steps=10,\n",
        "#         save_steps=500,\n",
        "#         report_to=None,\n",
        "#     )\n",
        "\n",
        "#     data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "#     trainer = Trainer(\n",
        "#         model=model,\n",
        "#         args=training_args,\n",
        "#         train_dataset=tokenized_dataset,\n",
        "#         data_collator=data_collator,\n",
        "#     )\n",
        "\n",
        "#     trainer.train()\n",
        "#     trainer.save_model()\n",
        "#     return model, tokenizer\n",
        "\n",
        "# # Usage in Colab:\n",
        "# # model, tokenizer = simple_train_with_streaming_dataset(ds)\n",
        "# '''\n",
        "\n",
        "# print(\"COPY THIS CODE TO GOOGLE COLAB:\")\n",
        "# print(\"=\" * 50)\n",
        "# print(colab_code)\n",
        "# print(\"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "017c113f77d045a9a87b2f3e33241313": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ef499c3403c4e4197839141a871887b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12406203214b41b499a3306a0f92993a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12bb74b2440a488cbf4c8b6b17912692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c55ef04870864ba48fba394f05600762",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_017c113f77d045a9a87b2f3e33241313",
            "value": 500
          }
        },
        "17e58f21f0444015a419c003ce4f0c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18f4a3f218ce4eddb95124cace94c9f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "199dad8114f647a7b20346e147644198": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d47ecaa0604f5083c51a971a001bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae6c82c22fff43039c93db25d0eafb0d",
            "placeholder": "​",
            "style": "IPY_MODEL_5529c96ffc5f461e982f5f41ac821587",
            "value": "Tokenizing dataset: 100%"
          }
        },
        "2b0f23fcb93943728092124f36de69f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b5c14dd1fd944f59171802d7579953f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f8ff8edb8f54df3b3a2a15068369968": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3797d944d3464c7eaf145c7522e71fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45900c73cef94ae6ac970907c0ca9ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aee6356d647145d2ae10e32a748552a5",
            "placeholder": "​",
            "style": "IPY_MODEL_6d17fa1771144488ba7cb0c96180e276",
            "value": "Tokenizing dataset: 100%"
          }
        },
        "4aa453fba7b44dc2b693b8f898612f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d7a8b33961f474d933996fefa9ccd95",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7099599574841d6a055d8406bd8bf41",
            "value": 500
          }
        },
        "5529c96ffc5f461e982f5f41ac821587": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5898dede3dad48329c33edfb04b61736": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12406203214b41b499a3306a0f92993a",
            "placeholder": "​",
            "style": "IPY_MODEL_17e58f21f0444015a419c003ce4f0c35",
            "value": " 50/50 [00:00&lt;00:00, 503.09 examples/s]"
          }
        },
        "5948a69ced1b4b698fe6c2c6845a7fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c3b3768f46c48c488a3aacab87bffb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ddbd4bf29134a749616ec77ab8e29f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb0c99a1690244cebc73fc33325989fd",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebc1acc527874dea931e501f04e4a79f",
            "value": 50
          }
        },
        "69cde02361bb4cc385d1643b80ba974c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a3f28431e5f4673baa851c4429ab036",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f8ff8edb8f54df3b3a2a15068369968",
            "value": 50
          }
        },
        "6d17fa1771144488ba7cb0c96180e276": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c51412832cb4959a597d55335585044": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d7a8b33961f474d933996fefa9ccd95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "814d7bdd5e02459dad0402cbad5ec660": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f63c369136448db9a825072ffa99a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e79f7d2fa1946449a783e0bcf0efc54",
              "IPY_MODEL_69cde02361bb4cc385d1643b80ba974c",
              "IPY_MODEL_d39950dfedca485b94dd42b87b6e41e9"
            ],
            "layout": "IPY_MODEL_a52b650931ed44ed910c512cc211718d"
          }
        },
        "932a0ac8cd704c25b98fc6d0dfc4b8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45900c73cef94ae6ac970907c0ca9ac0",
              "IPY_MODEL_5ddbd4bf29134a749616ec77ab8e29f8",
              "IPY_MODEL_5898dede3dad48329c33edfb04b61736"
            ],
            "layout": "IPY_MODEL_0ef499c3403c4e4197839141a871887b"
          }
        },
        "9a3f28431e5f4673baa851c4429ab036": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e79f7d2fa1946449a783e0bcf0efc54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7800398017e4349844036aee2e167b1",
            "placeholder": "​",
            "style": "IPY_MODEL_3797d944d3464c7eaf145c7522e71fe5",
            "value": "Map: 100%"
          }
        },
        "a52b650931ed44ed910c512cc211718d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7800398017e4349844036aee2e167b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae6c82c22fff43039c93db25d0eafb0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee6356d647145d2ae10e32a748552a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af877972c24b45d09bc7c975f12f8959": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_814d7bdd5e02459dad0402cbad5ec660",
            "placeholder": "​",
            "style": "IPY_MODEL_5c3b3768f46c48c488a3aacab87bffb7",
            "value": " 500/500 [00:00&lt;00:00, 654.99 examples/s]"
          }
        },
        "b4a594e83b31411f86b8ed2144d08ef7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7099599574841d6a055d8406bd8bf41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c55ef04870864ba48fba394f05600762": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d39950dfedca485b94dd42b87b6e41e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b5c14dd1fd944f59171802d7579953f",
            "placeholder": "​",
            "style": "IPY_MODEL_f8f652878b194d25b9196d2503e50d7e",
            "value": " 50/50 [00:00&lt;00:00, 1757.57 examples/s]"
          }
        },
        "de364798ceaa48f7ad0b68fceff6990b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f4a3f218ce4eddb95124cace94c9f7",
            "placeholder": "​",
            "style": "IPY_MODEL_5948a69ced1b4b698fe6c2c6845a7fe4",
            "value": " 500/500 [00:00&lt;00:00, 7443.62 examples/s]"
          }
        },
        "dfcdc0bdd32a476b83fa60bdbc41c233": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20d47ecaa0604f5083c51a971a001bf6",
              "IPY_MODEL_12bb74b2440a488cbf4c8b6b17912692",
              "IPY_MODEL_af877972c24b45d09bc7c975f12f8959"
            ],
            "layout": "IPY_MODEL_199dad8114f647a7b20346e147644198"
          }
        },
        "e05943536674457c8065faf190dfec9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e64bf29eecac4f67bff8f7dba7aa597b",
              "IPY_MODEL_4aa453fba7b44dc2b693b8f898612f4d",
              "IPY_MODEL_de364798ceaa48f7ad0b68fceff6990b"
            ],
            "layout": "IPY_MODEL_7c51412832cb4959a597d55335585044"
          }
        },
        "e64bf29eecac4f67bff8f7dba7aa597b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a594e83b31411f86b8ed2144d08ef7",
            "placeholder": "​",
            "style": "IPY_MODEL_2b0f23fcb93943728092124f36de69f6",
            "value": "Map: 100%"
          }
        },
        "eb0c99a1690244cebc73fc33325989fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc1acc527874dea931e501f04e4a79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8f652878b194d25b9196d2503e50d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
