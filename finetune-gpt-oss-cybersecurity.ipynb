{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9a4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.3.2-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (91 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-macosx_10_13_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-macosx_10_12_x86_64.whl.metadata (703 bytes)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp312-cp312-macosx_10_13_x86_64.whl (476 kB)\n",
      "Downloading multidict-6.6.3-cp312-cp312-macosx_10_13_x86_64.whl (45 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-macosx_10_13_x86_64.whl (91 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-macosx_10_13_x86_64.whl (47 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading numpy-2.3.2-cp312-cp312-macosx_14_0_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-macosx_10_13_x86_64.whl (43 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_x86_64.whl (32.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.7/32.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m6m0:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl (183 kB)\n",
      "Downloading regex-2025.7.34-cp312-cp312-macosx_10_13_x86_64.whl (290 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl (454 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.3.1-cp312-cp312-macosx_10_13_x86_64.whl (11.6 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl (31 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, safetensors, regex, pyyaml, pyarrow, propcache, numpy, multidict, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pandas, multiprocess, aiosignal, huggingface_hub, aiohttp, tokenizers, transformers, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/33\u001b[0m [datasets]/33\u001b[0m [datasets]ers]ub]er]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-4.0.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.7 huggingface_hub-0.34.4 idna-3.10 multidict-6.6.3 multiprocess-0.70.16 numpy-2.3.2 pandas-2.3.1 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.6.2 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.0 typing-extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install datasets huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b1fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exists!\n",
      "Dataset ID: Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\n",
      "Tags: ['task_categories:text-generation', 'task_categories:question-answering', 'language:en', 'license:apache-2.0', 'size_categories:10K<n<100K', 'format:json', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'region:us', 'cybersecurity', 'defensive-security', 'instruction-tuning', 'threat-intelligence', 'incident-response', 'security-operations']\n",
      "Number of files: 3\n",
      "\n",
      "Files in repository:\n",
      "  - .gitattributes\n",
      "  - CyberSec-Dataset_escaped.jsonl\n",
      "  - README.md\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, dataset_info\n",
    "import requests\n",
    "\n",
    "# Check the dataset repository structure\n",
    "api = HfApi()\n",
    "try:\n",
    "    info = api.dataset_info(\"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\")\n",
    "    print(\"Dataset exists!\")\n",
    "    print(f\"Dataset ID: {info.id}\")\n",
    "    print(f\"Tags: {info.tags}\")\n",
    "    print(f\"Number of files: {len(info.siblings) if info.siblings else 0}\")\n",
    "    \n",
    "    if info.siblings:\n",
    "        print(\"\\nFiles in repository:\")\n",
    "        for file in info.siblings:\n",
    "            print(f\"  - {file.rfilename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing dataset info: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a480d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Loading with explicit data_files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 53201 examples [00:01, 38475.06 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Method 1 SUCCESS!\n",
      "Dataset loaded with 53201 examples\n",
      "Features: {'system': Value('string'), 'user': Value('string'), 'assistant': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Method 1: Try loading with explicit data_files parameter\n",
    "print(\"Method 1: Loading with explicit data_files...\")\n",
    "try:\n",
    "    ds1 = load_dataset(\n",
    "        \"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\",\n",
    "        data_files=\"CyberSec-Dataset_escaped.jsonl\"\n",
    "    )\n",
    "    print(\"✅ Method 1 SUCCESS!\")\n",
    "    print(f\"Dataset loaded with {len(ds1['train'])} examples\")\n",
    "    print(f\"Features: {ds1['train'].features}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Method 1 FAILED: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6982d110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2: Loading with explicit format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 53201 examples [00:02, 19401.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Method 2 SUCCESS!\n",
      "Dataset loaded with 53201 examples\n",
      "Method 3: Loading with streaming...\n",
      "✅ Method 3 SUCCESS!\n",
      "Streaming dataset created successfully\n",
      "Streaming sample taken: 100 examples\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Try specifying the file format explicitly\n",
    "print(\"Method 2: Loading with explicit format...\")\n",
    "try:\n",
    "    ds2 = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=\"hf://datasets/Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset/CyberSec-Dataset_escaped.jsonl\"\n",
    "    )\n",
    "    print(\"✅ Method 2 SUCCESS!\")\n",
    "    print(f\"Dataset loaded with {len(ds2['train'])} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Method 2 FAILED: {e}\")\n",
    "    print()\n",
    "\n",
    "# Method 3: Try with streaming=True to avoid local caching issues\n",
    "print(\"Method 3: Loading with streaming...\")\n",
    "try:\n",
    "    ds3 = load_dataset(\n",
    "        \"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\",\n",
    "        data_files=\"CyberSec-Dataset_escaped.jsonl\",\n",
    "        streaming=True\n",
    "    )\n",
    "    print(\"✅ Method 3 SUCCESS!\")\n",
    "    print(\"Streaming dataset created successfully\")\n",
    "    # Convert to regular dataset for consistency\n",
    "    ds3_regular = ds3['train'].take(100)  # Take first 100 examples as test\n",
    "    print(f\"Streaming sample taken: {len(list(ds3_regular))} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Method 3 FAILED: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba727f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SAMPLE ===\n",
      "Total examples: 53201\n",
      "Features: ['system', 'user', 'assistant']\n",
      "\n",
      "Example 1:\n",
      "System: You are a highly specialized AI assistant for advanced cyber-defense whose mission is to deliver accurate, in-depth, actionable guidance on information-security principles—confidentiality, integrity, ...\n",
      "User: Analyze encrypted C2 channels using TLS. Discuss traffic analysis techniques to fingerprint malicious sessions.\n",
      "Assistant: Encrypted Command and Control (C2) channels utilizing Transport Layer Security (TLS) present significant challenges for network defenders, as traditional packet inspection methods cannot directly anal...\n",
      "\n",
      "=== BASIC STATISTICS ===\n",
      "Average system prompt length: 1085.0 chars\n",
      "Average user message length: 145.7 chars\n",
      "Average assistant response length: 2084.9 chars\n"
     ]
    }
   ],
   "source": [
    "# Display sample data from the successfully loaded dataset\n",
    "print(\"=== DATASET SAMPLE ===\")\n",
    "print(f\"Total examples: {len(ds1['train'])}\")\n",
    "print(f\"Features: {list(ds1['train'].features.keys())}\")\n",
    "print()\n",
    "\n",
    "# Show first example\n",
    "sample = ds1['train'][0]\n",
    "print(\"Example 1:\")\n",
    "print(f\"System: {sample['system'][:200]}...\" if len(sample['system']) > 200 else f\"System: {sample['system']}\")\n",
    "print(f\"User: {sample['user'][:200]}...\" if len(sample['user']) > 200 else f\"User: {sample['user']}\")\n",
    "print(f\"Assistant: {sample['assistant'][:200]}...\" if len(sample['assistant']) > 200 else f\"Assistant: {sample['assistant']}\")\n",
    "print()\n",
    "\n",
    "# Show data distribution\n",
    "print(\"=== BASIC STATISTICS ===\")\n",
    "sample_data = ds1['train'].select(range(min(1000, len(ds1['train']))))  # Sample first 1000 or total length\n",
    "system_lengths = [len(ex) for ex in sample_data['system']]\n",
    "user_lengths = [len(ex) for ex in sample_data['user']]\n",
    "assistant_lengths = [len(ex) for ex in sample_data['assistant']]\n",
    "\n",
    "print(f\"Average system prompt length: {sum(system_lengths)/len(system_lengths):.1f} chars\")\n",
    "print(f\"Average user message length: {sum(user_lengths)/len(user_lengths):.1f} chars\")\n",
    "print(f\"Average assistant response length: {sum(assistant_lengths)/len(assistant_lengths):.1f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43edf48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 4.36.0 not found\n"
     ]
    }
   ],
   "source": [
    "# First, let's fix the quantization issue by updating the model loading approach\n",
    "!pip install -q bitsandbytes accelerate\n",
    "\n",
    "# Also ensure we have compatible versions\n",
    "!pip install -q transformers>=4.36.0 peft>=0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c7765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/r7/wfm9wlx552s4w8c7qp7n015m0000gn/T/ipykernel_24090/2784349576.py\", line 2, in <module>\n",
      "    from transformers import BitsAndBytesConfig\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/utils/generic.py\", line 53, in <module>\n",
      "    import torch  # noqa: F401\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed CybersecurityFineTuner class created!\n"
     ]
    }
   ],
   "source": [
    "# Fix the model loading with proper BitsAndBytesConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "class CybersecurityFineTunerFixed:\n",
    "    \"\"\"Fixed fine-tuning class for cybersecurity-specific models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"Initialize the fine-tuner with configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.model_name = config.get('model_name', 'microsoft/DialoGPT-medium')  # Using a smaller model for testing\n",
    "        self.output_dir = config.get('output_dir', './cybersecurity-lora')\n",
    "        self.max_length = config.get('max_length', 512)\n",
    "        \n",
    "        # Check for CUDA availability\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"Load and configure the tokenizer.\"\"\"\n",
    "        print(f\"Loading tokenizer from {self.model_name}\")\n",
    "        \n",
    "        from transformers import AutoTokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Set pad token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            print(\"Set pad_token to eos_token\")\n",
    "        \n",
    "        print(\"Tokenizer loaded successfully\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the base model with proper quantization configuration.\"\"\"\n",
    "        print(f\"Loading model from {self.model_name}\")\n",
    "        \n",
    "        from transformers import AutoModelForCausalLM\n",
    "        \n",
    "        # Proper quantization configuration\n",
    "        if self.device == \"cuda\":\n",
    "            # Create proper BitsAndBytesConfig\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16,\n",
    "                bnb_8bit_use_double_quant=True,\n",
    "            )\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            print(\"Model loaded with 8-bit quantization\")\n",
    "        else:\n",
    "            # No quantization for CPU\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            print(\"Model loaded without quantization (CPU)\")\n",
    "        \n",
    "        print(f\"Model loaded on device: {next(self.model.parameters()).device}\")\n",
    "        \n",
    "    def configure_lora(self):\n",
    "        \"\"\"Configure and apply LoRA to the model.\"\"\"\n",
    "        print(\"Configuring LoRA\")\n",
    "        \n",
    "        from peft import LoraConfig, get_peft_model, TaskType\n",
    "        \n",
    "        # Get target modules based on the model architecture\n",
    "        target_modules = []\n",
    "        if \"gpt\" in self.model_name.lower():\n",
    "            target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    "        elif \"llama\" in self.model_name.lower():\n",
    "            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        else:\n",
    "            # Generic targets that work for most transformer models\n",
    "            target_modules = [\"q_proj\", \"v_proj\"]\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.config.get('lora_r', 16),\n",
    "            lora_alpha=self.config.get('lora_alpha', 32),\n",
    "            lora_dropout=self.config.get('lora_dropout', 0.1),\n",
    "            target_modules=target_modules,\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        self.model.print_trainable_parameters()\n",
    "        print(\"LoRA configuration applied\")\n",
    "    \n",
    "    def prepare_streaming_dataset(self, streaming_dataset):\n",
    "        \"\"\"Convert streaming dataset to regular dataset for training.\"\"\"\n",
    "        print(\"Converting streaming dataset to regular dataset...\")\n",
    "        \n",
    "        # Take a subset for training (you can adjust this number)\n",
    "        max_samples = self.config.get('max_samples', 1000)\n",
    "        \n",
    "        # Convert iterable dataset to list\n",
    "        train_data = []\n",
    "        count = 0\n",
    "        for example in streaming_dataset['train']:\n",
    "            if count >= max_samples:\n",
    "                break\n",
    "            train_data.append(example)\n",
    "            count += 1\n",
    "            \n",
    "            if count % 100 == 0:\n",
    "                print(f\"Processed {count} examples...\")\n",
    "        \n",
    "        # Convert to Hugging Face Dataset\n",
    "        from datasets import Dataset\n",
    "        dataset = Dataset.from_list(train_data)\n",
    "        \n",
    "        print(f\"Converted {len(dataset)} examples to regular dataset\")\n",
    "        return dataset\n",
    "    \n",
    "    def format_dataset(self, dataset):\n",
    "        \"\"\"Format the dataset for instruction following.\"\"\"\n",
    "        print(\"Formatting dataset\")\n",
    "        \n",
    "        def format_prompt(example):\n",
    "            \"\"\"Format examples for instruction following.\"\"\"\n",
    "            # Handle the cybersecurity dataset format (system, user, assistant)\n",
    "            if \"system\" in example and \"user\" in example and \"assistant\" in example:\n",
    "                # Create a chat-like format\n",
    "                prompt = f\"System: {example['system']}\\n\\nUser: {example['user']}\\n\\nAssistant: {example['assistant']}\"\n",
    "            elif \"instruction\" in example and \"response\" in example:\n",
    "                prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
    "            elif \"question\" in example and \"answer\" in example:\n",
    "                prompt = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n",
    "            else:\n",
    "                # Fallback: convert to string\n",
    "                prompt = str(example)\n",
    "            \n",
    "            return {\"text\": prompt}\n",
    "        \n",
    "        # Apply formatting\n",
    "        formatted_dataset = dataset.map(format_prompt)\n",
    "        print(\"Dataset formatting completed\")\n",
    "        return formatted_dataset\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        \"\"\"Tokenize the dataset for training.\"\"\"\n",
    "        print(\"Tokenizing dataset\")\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"Tokenize the dataset for training.\"\"\"\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=self.max_length,\n",
    "                return_overflowing_tokens=False,\n",
    "            )\n",
    "        \n",
    "        # Tokenize the dataset\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Tokenizing dataset\"\n",
    "        )\n",
    "        \n",
    "        # Create train/validation split\n",
    "        split_ratio = self.config.get('validation_split', 0.1)\n",
    "        if split_ratio > 0:\n",
    "            split_dataset = tokenized_dataset.train_test_split(test_size=split_ratio)\n",
    "            train_dataset = split_dataset[\"train\"]\n",
    "            eval_dataset = split_dataset[\"test\"]\n",
    "        else:\n",
    "            train_dataset = tokenized_dataset\n",
    "            eval_dataset = None\n",
    "        \n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        if eval_dataset:\n",
    "            print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "        \n",
    "        return train_dataset, eval_dataset\n",
    "    \n",
    "    def train_streaming(self, streaming_dataset):\n",
    "        \"\"\"Execute the complete training pipeline with streaming dataset.\"\"\"\n",
    "        print(\"Starting training pipeline with streaming dataset\")\n",
    "        \n",
    "        # Load components\n",
    "        self.load_tokenizer()\n",
    "        self.load_model()\n",
    "        self.configure_lora()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        dataset = self.prepare_streaming_dataset(streaming_dataset)\n",
    "        formatted_dataset = self.format_dataset(dataset)\n",
    "        train_dataset, eval_dataset = self.tokenize_dataset(formatted_dataset)\n",
    "        \n",
    "        # Create trainer\n",
    "        from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            per_device_train_batch_size=self.config.get('batch_size', 1),\n",
    "            per_device_eval_batch_size=self.config.get('eval_batch_size', 1),\n",
    "            gradient_accumulation_steps=self.config.get('gradient_accumulation_steps', 4),\n",
    "            num_train_epochs=self.config.get('epochs', 1),\n",
    "            learning_rate=self.config.get('learning_rate', 2e-4),\n",
    "            fp16=self.device == \"cuda\",\n",
    "            logging_steps=self.config.get('logging_steps', 10),\n",
    "            eval_steps=self.config.get('eval_steps', 100),\n",
    "            save_steps=self.config.get('save_steps', 500),\n",
    "            evaluation_strategy=\"steps\" if eval_dataset else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            load_best_model_at_end=eval_dataset is not None,\n",
    "            warmup_steps=self.config.get('warmup_steps', 50),\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            report_to=None,\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        print(\"Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "\n",
    "print(\"Fixed CybersecurityFineTuner class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f44eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration created for fixed fine-tuner\n",
      "Will use model: microsoft/DialoGPT-medium\n",
      "Max samples for training: 500\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Create a configuration for the fixed fine-tuner\n",
    "config_fixed = {\n",
    "    \"model_name\": \"microsoft/DialoGPT-medium\",  # Using a smaller, more compatible model\n",
    "    \"output_dir\": \"./cybersecurity-lora-fixed\",\n",
    "    \"max_length\": 512,\n",
    "    \"max_samples\": 500,  # Limit samples for testing\n",
    "    \"validation_split\": 0.1,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"batch_size\": 1,\n",
    "    \"eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"epochs\": 1,  # Just 1 epoch for testing\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 100,\n",
    "    \"warmup_steps\": 20,\n",
    "}\n",
    "\n",
    "print(\"Configuration created for fixed fine-tuner\")\n",
    "print(f\"Will use model: {config_fixed['model_name']}\")\n",
    "print(f\"Max samples for training: {config_fixed['max_samples']}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a243aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the fixed fine-tuner...\n",
      "Using streaming dataset: <class 'datasets.dataset_dict.IterableDatasetDict'>\n",
      "Dataset splits: ['train']\n",
      "Using device: cpu\n",
      "\n",
      "=== Testing dataset preparation ===\n",
      "Converting streaming dataset to regular dataset...\n",
      "Processed 100 examples...\n",
      "Processed 200 examples...\n",
      "Processed 300 examples...\n",
      "Processed 400 examples...\n",
      "Processed 500 examples...\n",
      "Converted 500 examples to regular dataset\n",
      "Dataset conversion successful! Got 500 examples\n",
      "\n",
      "Sample data fields: ['system', 'user', 'assistant']\n",
      "Sample system (first 100 chars): You are a highly specialized AI assistant for advanced cyber-defense whose mission is to deliver acc...\n",
      "Sample user (first 100 chars): Analyze encrypted C2 channels using TLS. Discuss traffic analysis techniques to fingerprint maliciou...\n",
      "Sample assistant (first 100 chars): Encrypted Command and Control (C2) channels utilizing Transport Layer Security (TLS) present signifi...\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed fine-tuner with your streaming dataset\n",
    "print(\"Testing the fixed fine-tuner...\")\n",
    "print(f\"Using streaming dataset: {type(ds)}\")\n",
    "print(f\"Dataset splits: {list(ds.keys())}\")\n",
    "\n",
    "# Create the fixed fine-tuner\n",
    "fine_tuner_fixed = CybersecurityFineTunerFixed(config_fixed)\n",
    "\n",
    "# Let's first test the dataset preparation without training\n",
    "print(\"\\n=== Testing dataset preparation ===\")\n",
    "test_dataset = fine_tuner_fixed.prepare_streaming_dataset(ds)\n",
    "print(f\"Dataset conversion successful! Got {len(test_dataset)} examples\")\n",
    "\n",
    "# Show a sample\n",
    "if len(test_dataset) > 0:\n",
    "    sample = test_dataset[0]\n",
    "    print(f\"\\nSample data fields: {list(sample.keys())}\")\n",
    "    print(f\"Sample system (first 100 chars): {sample['system'][:100]}...\")\n",
    "    print(f\"Sample user (first 100 chars): {sample['user'][:100]}...\")\n",
    "    print(f\"Sample assistant (first 100 chars): {sample['assistant'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d9a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting actual training ===\n",
      "Creating fine-tuner with small config for testing...\n",
      "Using device: cpu\n",
      "Starting training (this may take a few minutes even with small dataset)...\n",
      "Starting training pipeline with streaming dataset\n",
      "Loading tokenizer from microsoft/DialoGPT-medium\n",
      "Set pad_token to eos_token\n",
      "Tokenizer loaded successfully\n",
      "Loading model from microsoft/DialoGPT-medium\n",
      "❌ Training failed with error: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/r7/wfm9wlx552s4w8c7qp7n015m0000gn/T/ipykernel_24090/293461785.py\", line 22, in <module>\n",
      "    fine_tuner_small.train_streaming(ds)\n",
      "  File \"/var/folders/r7/wfm9wlx552s4w8c7qp7n015m0000gn/T/ipykernel_24090/2784349576.py\", line 197, in train_streaming\n",
      "    self.load_model()\n",
      "  File \"/var/folders/r7/wfm9wlx552s4w8c7qp7n015m0000gn/T/ipykernel_24090/2784349576.py\", line 62, in load_model\n",
      "    self.model = AutoModelForCausalLM.from_pretrained(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 316, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 5061, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 5327, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 561, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/Users/tilmann/Documents/GitHub/finetune-gpt-oss-cybersecurity/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1622, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n"
     ]
    }
   ],
   "source": [
    "# Now let's run the actual training with a very small setup for testing\n",
    "print(\"=== Starting actual training ===\")\n",
    "\n",
    "# Create a smaller config for quick testing\n",
    "config_small = config_fixed.copy()\n",
    "config_small.update({\n",
    "    \"max_samples\": 50,  # Very small for quick test\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"epochs\": 1,\n",
    "    \"logging_steps\": 5,\n",
    "    \"eval_steps\": 20,\n",
    "    \"save_steps\": 50,\n",
    "    \"warmup_steps\": 5,\n",
    "})\n",
    "\n",
    "print(\"Creating fine-tuner with small config for testing...\")\n",
    "fine_tuner_small = CybersecurityFineTunerFixed(config_small)\n",
    "\n",
    "print(\"Starting training (this may take a few minutes even with small dataset)...\")\n",
    "try:\n",
    "    fine_tuner_small.train_streaming(ds)\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b48d41bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SOLUTIONS FOR GOOGLE COLAB ===\n",
      "\n",
      "1. TORCH VERSION ISSUE:\n",
      "In Google Colab, run this first:\n",
      "!pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
      "Then restart runtime!\n",
      "\n",
      "2. QUANTIZATION ISSUE FIX:\n",
      "For Google Colab with CUDA, use this updated code:\n",
      "\n",
      "COPY THIS CODE TO GOOGLE COLAB:\n",
      "==================================================\n",
      "\n",
      "# FOR GOOGLE COLAB - Run this in your Colab notebook:\n",
      "\n",
      "# 1. First install/upgrade packages\n",
      "!pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
      "!pip install -q transformers>=4.40.0 peft>=0.7.0 bitsandbytes accelerate datasets\n",
      "\n",
      "# 2. Restart runtime, then use this code:\n",
      "\n",
      "from transformers import BitsAndBytesConfig\n",
      "import torch\n",
      "\n",
      "# Fixed quantization config for newer transformers\n",
      "def create_quantization_config():\n",
      "    return BitsAndBytesConfig(\n",
      "        load_in_8bit=True,\n",
      "        bnb_8bit_compute_dtype=torch.float16,\n",
      "        bnb_8bit_quant_type=\"nf8\",\n",
      "        bnb_8bit_use_double_quant=True,\n",
      "    )\n",
      "\n",
      "# Updated model loading for Colab\n",
      "def load_model_colab(model_name):\n",
      "    from transformers import AutoModelForCausalLM\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        quantization_config = create_quantization_config()\n",
      "        model = AutoModelForCausalLM.from_pretrained(\n",
      "            model_name,\n",
      "            quantization_config=quantization_config,\n",
      "            device_map=\"auto\",\n",
      "            torch_dtype=torch.float16,\n",
      "            trust_remote_code=True,\n",
      "            use_safetensors=True,  # This helps with the security issue\n",
      "        )\n",
      "    else:\n",
      "        model = AutoModelForCausalLM.from_pretrained(\n",
      "            model_name,\n",
      "            torch_dtype=torch.float16,\n",
      "            trust_remote_code=True,\n",
      "            use_safetensors=True,\n",
      "        )\n",
      "    return model\n",
      "\n",
      "# 3. For your cybersecurity dataset, use this simple approach:\n",
      "def simple_train_with_streaming_dataset(ds, model_name=\"microsoft/DialoGPT-medium\"):\n",
      "    from datasets import Dataset\n",
      "    from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
      "    from transformers import DataCollatorForLanguageModeling\n",
      "    from peft import LoraConfig, get_peft_model, TaskType\n",
      "\n",
      "    # Convert streaming to regular dataset (first 1000 examples)\n",
      "    train_data = []\n",
      "    for i, example in enumerate(ds['train']):\n",
      "        if i >= 1000:  # Limit for memory\n",
      "            break\n",
      "        # Format the data\n",
      "        text = f\"System: {example['system']}\\n\\nUser: {example['user']}\\n\\nAssistant: {example['assistant']}\"\n",
      "        train_data.append({\"text\": text})\n",
      "\n",
      "    dataset = Dataset.from_list(train_data)\n",
      "    print(f\"Prepared {len(dataset)} examples\")\n",
      "\n",
      "    # Load tokenizer and model\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "    if tokenizer.pad_token is None:\n",
      "        tokenizer.pad_token = tokenizer.eos_token\n",
      "\n",
      "    model = load_model_colab(model_name)\n",
      "\n",
      "    # Apply LoRA\n",
      "    lora_config = LoraConfig(\n",
      "        task_type=TaskType.CAUSAL_LM,\n",
      "        r=16,\n",
      "        lora_alpha=32,\n",
      "        lora_dropout=0.1,\n",
      "        target_modules=[\"c_attn\", \"c_proj\"] if \"gpt\" in model_name.lower() else [\"q_proj\", \"v_proj\"],\n",
      "    )\n",
      "    model = get_peft_model(model, lora_config)\n",
      "\n",
      "    # Tokenize dataset\n",
      "    def tokenize_function(examples):\n",
      "        return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=512)\n",
      "\n",
      "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
      "\n",
      "    # Training\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=\"./cybersecurity-lora\",\n",
      "        per_device_train_batch_size=1,\n",
      "        gradient_accumulation_steps=4,\n",
      "        num_train_epochs=1,\n",
      "        learning_rate=2e-4,\n",
      "        fp16=True if torch.cuda.is_available() else False,\n",
      "        logging_steps=10,\n",
      "        save_steps=500,\n",
      "        report_to=None,\n",
      "    )\n",
      "\n",
      "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=tokenized_dataset,\n",
      "        data_collator=data_collator,\n",
      "    )\n",
      "\n",
      "    trainer.train()\n",
      "    trainer.save_model()\n",
      "    return model, tokenizer\n",
      "\n",
      "# Usage in Colab:\n",
      "# model, tokenizer = simple_train_with_streaming_dataset(ds)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION FOR GOOGLE COLAB\n",
    "print(\"=== SOLUTIONS FOR GOOGLE COLAB ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. TORCH VERSION ISSUE:\")\n",
    "print(\"In Google Colab, run this first:\")\n",
    "print(\"!pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "print(\"Then restart runtime!\")\n",
    "print()\n",
    "\n",
    "print(\"2. QUANTIZATION ISSUE FIX:\")\n",
    "print(\"For Google Colab with CUDA, use this updated code:\")\n",
    "print()\n",
    "\n",
    "# Provide the corrected Colab-specific code\n",
    "colab_code = '''\n",
    "# FOR GOOGLE COLAB - Run this in your Colab notebook:\n",
    "\n",
    "# 1. First install/upgrade packages\n",
    "!pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers>=4.40.0 peft>=0.7.0 bitsandbytes accelerate datasets\n",
    "\n",
    "# 2. Restart runtime, then use this code:\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Fixed quantization config for newer transformers\n",
    "def create_quantization_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "        bnb_8bit_quant_type=\"nf8\",\n",
    "        bnb_8bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "# Updated model loading for Colab\n",
    "def load_model_colab(model_name):\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        quantization_config = create_quantization_config()\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            use_safetensors=True,  # This helps with the security issue\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# 3. For your cybersecurity dataset, use this simple approach:\n",
    "def simple_train_with_streaming_dataset(ds, model_name=\"microsoft/DialoGPT-medium\"):\n",
    "    from datasets import Dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "    from transformers import DataCollatorForLanguageModeling\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    \n",
    "    # Convert streaming to regular dataset (first 1000 examples)\n",
    "    train_data = []\n",
    "    for i, example in enumerate(ds['train']):\n",
    "        if i >= 1000:  # Limit for memory\n",
    "            break\n",
    "        # Format the data\n",
    "        text = f\"System: {example['system']}\\\\n\\\\nUser: {example['user']}\\\\n\\\\nAssistant: {example['assistant']}\"\n",
    "        train_data.append({\"text\": text})\n",
    "    \n",
    "    dataset = Dataset.from_list(train_data)\n",
    "    print(f\"Prepared {len(dataset)} examples\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = load_model_colab(model_name)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"] if \"gpt\" in model_name.lower() else [\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=512)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./cybersecurity-lora\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        report_to=None,\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Usage in Colab:\n",
    "# model, tokenizer = simple_train_with_streaming_dataset(ds)\n",
    "'''\n",
    "\n",
    "print(\"COPY THIS CODE TO GOOGLE COLAB:\")\n",
    "print(\"=\" * 50)\n",
    "print(colab_code)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48008d8-c8db-4e1a-90b7-8e9e18e3aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q git+https://github.com/huggingface/transformers triton==3.4 git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "!pip uninstall -q torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf795e2-337b-4ea1-b4eb-a044c63c4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3351202e-70c5-49ff-8d24-ac3b37f9100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b19b7de-b9ef-4e34-80b9-2b63ae24f762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 19:58:15.997567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftModel, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5fbe3d9-be9b-4901-9b96-1f0efd19b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377c79a1-f06b-4c8f-bb75-1d94ae86e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CybersecurityFineTuner:\n",
    "    \"\"\"Fine-tuning class for cybersecurity-specific GPT-OSS20B model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"Initialize the fine-tuner with configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.model_name = config.get('model_name', 'openai/gpt-oss-20b')\n",
    "        self.output_dir = config.get('output_dir', './gpt-oss-cybersecurity-lora')\n",
    "        self.max_length = config.get('max_length', 512)\n",
    "        \n",
    "        # Check for CUDA availability\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if self.device == \"cpu\":\n",
    "            logger.warning(\"CUDA not available. Training will be very slow on CPU.\")\n",
    "        \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.dataset = None\n",
    "        \n",
    "    def load_tokenizer(self) -> None:\n",
    "        \"\"\"Load and configure the tokenizer.\"\"\"\n",
    "        logger.info(f\"Loading tokenizer from {self.model_name}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Set pad token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            logger.info(\"Set pad_token to eos_token\")\n",
    "        \n",
    "        logger.info(\"Tokenizer loaded successfully\")\n",
    "    \n",
    "    def load_model(self) -> None:\n",
    "        \"\"\"Load the base model with quantization.\"\"\"\n",
    "        logger.info(f\"Loading model from {self.model_name}\")\n",
    "        \n",
    "        # Model loading configuration\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "        \n",
    "        # Add quantization if CUDA is available\n",
    "        if self.device == \"cuda\":\n",
    "            model_kwargs[\"load_in_8bit\"] = True\n",
    "            logger.info(\"Using 8-bit quantization\")\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Model loaded on device: {self.model.device}\")\n",
    "        \n",
    "    def configure_lora(self) -> None:\n",
    "        \"\"\"Configure and apply LoRA to the model.\"\"\"\n",
    "        logger.info(\"Configuring LoRA\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.config.get('lora_r', 16),\n",
    "            lora_alpha=self.config.get('lora_alpha', 32),\n",
    "            lora_dropout=self.config.get('lora_dropout', 0.1),\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        self.model.print_trainable_parameters()\n",
    "        logger.info(\"LoRA configuration applied\")\n",
    "    \n",
    "    def load_cybersecurity_dataset(self, dataset_path: str) -> None:\n",
    "        \"\"\"Load and prepare the cybersecurity dataset.\"\"\"\n",
    "        logger.info(f\"Loading dataset from {dataset_path}\")\n",
    "        \n",
    "        if dataset_path.endswith('.jsonl'):\n",
    "            # Load JSONL file\n",
    "            data = []\n",
    "            with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "            \n",
    "            self.dataset = Dataset.from_list(data)\n",
    "            \n",
    "        elif dataset_path.endswith('.json'):\n",
    "            # Load JSON file\n",
    "            with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle different JSON structures\n",
    "            if isinstance(data, list):\n",
    "                self.dataset = Dataset.from_list(data)\n",
    "            elif isinstance(data, dict) and 'data' in data:\n",
    "                self.dataset = Dataset.from_list(data['data'])\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported JSON structure\")\n",
    "                \n",
    "        else:\n",
    "            # Try loading from Hugging Face datasets\n",
    "            self.dataset = load_dataset(dataset_path, split='train')\n",
    "        \n",
    "        logger.info(f\"Dataset loaded with {len(self.dataset)} examples\")\n",
    "    \n",
    "    def format_dataset(self) -> None:\n",
    "        \"\"\"Format the dataset for instruction following.\"\"\"\n",
    "        logger.info(\"Formatting dataset\")\n",
    "        \n",
    "        def format_prompt(example):\n",
    "            \"\"\"Format examples for instruction following.\"\"\"\n",
    "            # Try different common field combinations\n",
    "            if \"instruction\" in example and \"response\" in example:\n",
    "                prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
    "            elif \"question\" in example and \"answer\" in example:\n",
    "                prompt = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n",
    "            elif \"input\" in example and \"output\" in example:\n",
    "                prompt = f\"### Input:\\n{example['input']}\\n\\n### Output:\\n{example['output']}\"\n",
    "            elif \"prompt\" in example and \"completion\" in example:\n",
    "                prompt = f\"### Instruction:\\n{example['prompt']}\\n\\n### Response:\\n{example['completion']}\"\n",
    "            elif \"text\" in example:\n",
    "                # Use as-is if already formatted\n",
    "                prompt = example['text']\n",
    "            else:\n",
    "                # Fallback: use first two string fields found\n",
    "                string_fields = [k for k, v in example.items() if isinstance(v, str)]\n",
    "                if len(string_fields) >= 2:\n",
    "                    prompt = f\"### Input:\\n{example[string_fields[0]]}\\n\\n### Output:\\n{example[string_fields[1]]}\"\n",
    "                else:\n",
    "                    logger.warning(f\"Could not format example: {example.keys()}\")\n",
    "                    prompt = str(example)\n",
    "            \n",
    "            return {\"text\": prompt}\n",
    "        \n",
    "        # Apply formatting\n",
    "        self.dataset = self.dataset.map(format_prompt)\n",
    "        logger.info(\"Dataset formatting completed\")\n",
    "    \n",
    "    def tokenize_dataset(self) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Tokenize the dataset and create train/validation splits.\"\"\"\n",
    "        logger.info(\"Tokenizing dataset\")\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"Tokenize the dataset for training.\"\"\"\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=self.max_length,\n",
    "                return_overflowing_tokens=False,\n",
    "            )\n",
    "        \n",
    "        # Tokenize the dataset\n",
    "        tokenized_dataset = self.dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.dataset.column_names,\n",
    "            desc=\"Tokenizing dataset\"\n",
    "        )\n",
    "        \n",
    "        # Create train/validation split\n",
    "        split_ratio = self.config.get('validation_split', 0.1)\n",
    "        if split_ratio > 0:\n",
    "            split_dataset = tokenized_dataset.train_test_split(test_size=split_ratio)\n",
    "            train_dataset = split_dataset[\"train\"]\n",
    "            eval_dataset = split_dataset[\"test\"]\n",
    "        else:\n",
    "            train_dataset = tokenized_dataset\n",
    "            eval_dataset = None\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "        if eval_dataset:\n",
    "            logger.info(f\"Validation samples: {len(eval_dataset)}\")\n",
    "        \n",
    "        return train_dataset, eval_dataset\n",
    "    \n",
    "    def create_trainer(self, train_dataset: Dataset, eval_dataset: Optional[Dataset] = None) -> Trainer:\n",
    "        \"\"\"Create and configure the trainer.\"\"\"\n",
    "        logger.info(\"Creating trainer\")\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            per_device_train_batch_size=self.config.get('batch_size', 2),\n",
    "            per_device_eval_batch_size=self.config.get('eval_batch_size', 2),\n",
    "            gradient_accumulation_steps=self.config.get('gradient_accumulation_steps', 8),\n",
    "            num_train_epochs=self.config.get('epochs', 3),\n",
    "            learning_rate=self.config.get('learning_rate', 2e-4),\n",
    "            fp16=self.device == \"cuda\",\n",
    "            logging_steps=self.config.get('logging_steps', 10),\n",
    "            eval_steps=self.config.get('eval_steps', 100),\n",
    "            save_steps=self.config.get('save_steps', 500),\n",
    "            evaluation_strategy=\"steps\" if eval_dataset else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            load_best_model_at_end=eval_dataset is not None,\n",
    "            metric_for_best_model=\"eval_loss\" if eval_dataset else None,\n",
    "            greater_is_better=False,\n",
    "            warmup_steps=self.config.get('warmup_steps', 100),\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            report_to=None,\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def train(self, dataset_path: str) -> None:\n",
    "        \"\"\"Execute the complete training pipeline.\"\"\"\n",
    "        logger.info(\"Starting training pipeline\")\n",
    "        \n",
    "        # Load components\n",
    "        self.load_tokenizer()\n",
    "        self.load_model()\n",
    "        self.configure_lora()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        self.load_cybersecurity_dataset(dataset_path)\n",
    "        self.format_dataset()\n",
    "        train_dataset, eval_dataset = self.tokenize_dataset()\n",
    "        \n",
    "        # Create and run trainer\n",
    "        trainer = self.create_trainer(train_dataset, eval_dataset)\n",
    "        \n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        logger.info(\"Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "        \n",
    "        logger.info(\"Training completed successfully!\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_length: int = 256) -> str:\n",
    "        \"\"\"Generate response using the fine-tuned model.\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise ValueError(\"Model and tokenizer must be loaded first\")\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=len(inputs['input_ids'][0]) + max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode and return response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the generated part\n",
    "        generated_text = response[len(formatted_prompt):]\n",
    "        \n",
    "        return generated_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e670f8f-750f-439f-ab1c-66e89d9de520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fine_tuned_model(base_model_path: str, lora_adapter_path: str) -> Tuple[PeftModel, AutoTokenizer]:\n",
    "    \"\"\"Load a fine-tuned model for inference.\"\"\"\n",
    "    logger.info(f\"Loading fine-tuned model from {lora_adapter_path}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_adapter_path)\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def create_sample_config() -> Dict:\n",
    "    \"\"\"Create a sample configuration.\"\"\"\n",
    "    return {\n",
    "        \"model_name\": \"openai/gpt-oss-20b\",\n",
    "        \"output_dir\": \"./gpt-oss-cybersecurity-lora\",\n",
    "        \"max_length\": 512,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"batch_size\": 2,\n",
    "        \"eval_batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"logging_steps\": 10,\n",
    "        \"eval_steps\": 100,\n",
    "        \"save_steps\": 500,\n",
    "        \"warmup_steps\": 100,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_sample_dataset(output_path: str) -> None:\n",
    "    \"\"\"Create a sample cybersecurity dataset for testing.\"\"\"\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"instruction\": \"What are the key indicators of a potential SQL injection attack?\",\n",
    "            \"response\": \"Key indicators of SQL injection attacks include: 1) Unusual database queries in logs, 2) Error messages revealing database structure, 3) Unexpected application behavior when special characters are entered, 4) Slow database performance, 5) Authentication bypasses, and 6) Data exfiltration patterns in network traffic.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How should an organization respond to a ransomware attack?\",\n",
    "            \"response\": \"Ransomware response steps: 1) Immediately isolate affected systems, 2) Activate incident response team, 3) Assess scope of infection, 4) Preserve evidence, 5) Restore from clean backups if available, 6) Report to law enforcement and relevant authorities, 7) Communicate with stakeholders, 8) Conduct post-incident analysis, and 9) Improve security measures based on lessons learned.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What is the principle of least privilege in cybersecurity?\",\n",
    "            \"response\": \"The principle of least privilege is a security concept where users, applications, and systems are granted only the minimum access rights necessary to perform their functions. This reduces the attack surface by limiting potential damage if credentials are compromised and helps prevent lateral movement in case of a breach.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in sample_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    logger.info(f\"Sample dataset created at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344be7aa-9f8a-47bf-9f74-c30982d493d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG] --dataset DATASET\n",
      "                             [--create-sample-config]\n",
      "                             [--create-sample-dataset CREATE_SAMPLE_DATASET]\n",
      "                             [--inference INFERENCE] [--prompt PROMPT]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --dataset\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to handle command line arguments and execute training.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Fine-tune GPT-OSS20B for cybersecurity')\n",
    "    parser.add_argument('--config', type=str, help='Path to configuration YAML file')\n",
    "    parser.add_argument('--dataset', type=str, required=True, help='Path to dataset file')\n",
    "    parser.add_argument('--create-sample-config', action='store_true', \n",
    "                       help='Create sample configuration file')\n",
    "    parser.add_argument('--create-sample-dataset', type=str,\n",
    "                       help='Create sample dataset at specified path')\n",
    "    parser.add_argument('--inference', type=str, help='Path to fine-tuned model for inference')\n",
    "    parser.add_argument('--prompt', type=str, help='Prompt for inference')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create sample configuration\n",
    "    if args.create_sample_config:\n",
    "        config = create_sample_config()\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        logger.info(\"Sample configuration created as 'config.yaml'\")\n",
    "        return\n",
    "    \n",
    "    # Create sample dataset\n",
    "    if args.create_sample_dataset:\n",
    "        create_sample_dataset(args.create_sample_dataset)\n",
    "        return\n",
    "    \n",
    "    # Inference mode\n",
    "    if args.inference:\n",
    "        if not args.prompt:\n",
    "            logger.error(\"--prompt is required for inference\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        model, tokenizer = load_fine_tuned_model(\"openai/gpt-oss-20b\", args.inference)\n",
    "        \n",
    "        # Create temporary fine-tuner for inference\n",
    "        config = {\"model_name\": \"openai/gpt-oss-20b\"}\n",
    "        fine_tuner = CybersecurityFineTuner(config)\n",
    "        fine_tuner.model = model\n",
    "        fine_tuner.tokenizer = tokenizer\n",
    "        \n",
    "        response = fine_tuner.generate_response(args.prompt)\n",
    "        print(f\"Query: {args.prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        return\n",
    "    \n",
    "    # Training mode\n",
    "    if not args.dataset:\n",
    "        logger.error(\"--dataset is required for training\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Load configuration\n",
    "    if args.config:\n",
    "        with open(args.config, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    else:\n",
    "        config = create_sample_config()\n",
    "        logger.info(\"Using default configuration\")\n",
    "    \n",
    "    # Create and run fine-tuner\n",
    "    fine_tuner = CybersecurityFineTuner(config)\n",
    "    fine_tuner.train(args.dataset)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba7a45c-1c91-4b6b-bd9a-34da295c138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = create_sample_config()\n",
    "# with open('config.yaml', 'w') as f:\n",
    "#     yaml.dump(config, f, default_flow_style=False)\n",
    "# logger.info(\"Sample configuration created as 'config.yaml'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f4ea1c2-fec7-4b59-9fff-05ac0d545e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = yaml.safe_load(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0daa7e-f736-4ba0-88e1-164c134f2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "        \"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\",\n",
    "        data_files=\"CyberSec-Dataset_escaped.jsonl\",\n",
    "        streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b175a0c-5409-46e7-a826-600bd6be692c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CybersecurityFineTuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create and run fine-tuner\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fine_tuner = \u001b[43mCybersecurityFineTuner\u001b[49m(config)\n\u001b[32m      3\u001b[39m fine_tuner.train(ds)\n",
      "\u001b[31mNameError\u001b[39m: name 'CybersecurityFineTuner' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and run fine-tuner\n",
    "fine_tuner = CybersecurityFineTuner(config)\n",
    "fine_tuner.train(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3788cf4-72ca-4cf6-9245-c87ee5a0b7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 20:10:16,025 - __main__ - INFO - Using default configuration\n",
      "2025-08-09 20:10:16,028 - __main__ - WARNING - CUDA not available. Training will be very slow on CPU.\n",
      "2025-08-09 20:10:16,029 - __main__ - INFO - Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "config = create_sample_config()\n",
    "logger.info(\"Using default configuration\")\n",
    "    \n",
    "# Create and run fine-tuner\n",
    "fine_tuner = CybersecurityFineTuner(config)\n",
    "# fine_tuner.train(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "968d948c-0bee-4900-99d3-b5fc5a092261",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CybersecurityFineTuner.train() missing 1 required positional argument: 'dataset_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfine_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: CybersecurityFineTuner.train() missing 1 required positional argument: 'dataset_path'"
     ]
    }
   ],
   "source": [
    "fine_tuner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a08948-fc4c-4d5f-bc68-a6223cd6bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
